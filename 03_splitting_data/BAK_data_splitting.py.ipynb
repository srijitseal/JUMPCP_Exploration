{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd0f1de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxcast\n",
      "JUMPCP_compounds\n",
      "BBBP\n",
      "sider\n",
      "tox21\n",
      "muv\n",
      "HIV\n",
      "DILIst\n",
      "PK_Lombardo\n",
      "bace\n",
      "clintox\n",
      "toxcast\n",
      "JUMPCP_compounds\n",
      "BBBP\n",
      "sider\n",
      "tox21\n",
      "muv\n",
      "HIV\n",
      "DILIst\n",
      "PK_Lombardo\n",
      "bace\n",
      "clintox\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ss2686/JUMPCP')\n",
    "\n",
    "from scripts.stratified_split_helper import stratified_data_split\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Loading the dictionary\n",
    "\n",
    "pickle_file_path = '../02_Explore_JUMP_compounds/activity_columns_mapping_selected.pkl'\n",
    "\n",
    "# Loading the dictionary\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    activity_columns_mapping = pickle.load(file)\n",
    "    \n",
    "def process_datasets(directory='../data/processed_overlap/'):\n",
    "    datasets = {}\n",
    "    splits = {}\n",
    "\n",
    "    # Load datasets from given directory\n",
    "    for foldername in os.listdir(directory):\n",
    "        \n",
    "        if not foldername.startswith('.'):  # Ignore folders starting with a dot\n",
    "            \n",
    "            print(foldername)\n",
    "            file_path = os.path.join(directory, foldername, f\"{foldername}_processed_overlap.csv.gz\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                datasets[foldername] = pd.read_csv(file_path, compression='gzip')\n",
    "            else:\n",
    "                print(f\"No matching file found for folder: {foldername}\")\n",
    "\n",
    "    # Stratified split for each dataset and each activity column\n",
    "    for name, df in datasets.items():\n",
    "        activity_cols = activity_columns_mapping.get(name, [])\n",
    "\n",
    "        dataset_splits = {}\n",
    "        for col in activity_cols:\n",
    "            #print(name)\n",
    "            #print(col)\n",
    "            train_df, test_df = stratified_data_split(df, activity_col= col, dataset_name=name)\n",
    "            # Filter the columns for saving\n",
    "            cols_to_keep = ['Standardized_SMILES', 'Standardized_InChI', col]\n",
    "            train_df = train_df[cols_to_keep]\n",
    "            test_df = test_df[cols_to_keep]\n",
    "            \n",
    "            dataset_splits[col] = {'train': train_df, 'test': test_df}\n",
    "\n",
    "        splits[name] = dataset_splits\n",
    "\n",
    "    # Save the splits to new directories\n",
    "    output_dir = \"../data/processed_splits/\"\n",
    "    # Ensure the directory exists, if not, create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Iterate through each dataset name, activities, and their corresponding splits\n",
    "    for dataset_name, activities in splits.items():\n",
    "        print(dataset_name)\n",
    "        for activity, split_data in activities.items():\n",
    "            for data_type, split_df in split_data.items():\n",
    "                # Create a path for the dataset if it doesn't exist\n",
    "                dataset_path = os.path.join(output_dir, dataset_name)\n",
    "                if not os.path.exists(dataset_path):\n",
    "                    os.makedirs(dataset_path)\n",
    "\n",
    "                # Define the full output path for the split dataframe\n",
    "                output_path = os.path.join(dataset_path, f\"{activity}_{data_type}.csv.gz\")\n",
    "\n",
    "                # Save the dataframe\n",
    "                split_df.to_csv(output_path, index=False, compression='gzip')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
